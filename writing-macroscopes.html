<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Writing Macroscopes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Writing Macroscopes">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="Writing Macroscopes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Writing Macroscopes" />
  
  
  

<meta name="author" content="Aaron Beveridge">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="writing-to-scale.html">
<link rel="next" href="network-descriptions.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="writing-to-scale.html"><a href="writing-to-scale.html"><i class="fa fa-check"></i><b>2</b> Writing to Scale</a></li>
<li class="chapter" data-level="3" data-path="writing-macroscopes.html"><a href="writing-macroscopes.html"><i class="fa fa-check"></i><b>3</b> Writing Macroscopes</a><ul>
<li class="chapter" data-level="3.1" data-path="writing-macroscopes.html"><a href="writing-macroscopes.html#social-network-data"><i class="fa fa-check"></i><b>3.1</b> Social Network Data</a></li>
<li class="chapter" data-level="3.2" data-path="writing-macroscopes.html"><a href="writing-macroscopes.html#data-access-and-collection"><i class="fa fa-check"></i><b>3.2</b> Data Access and Collection</a></li>
<li class="chapter" data-level="3.3" data-path="writing-macroscopes.html"><a href="writing-macroscopes.html#circulation-analytics"><i class="fa fa-check"></i><b>3.3</b> Circulation Analytics</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="network-descriptions.html"><a href="network-descriptions.html"><i class="fa fa-check"></i><b>4</b> Network Descriptions</a></li>
<li class="chapter" data-level="5" data-path="network-effects.html"><a href="network-effects.html"><i class="fa fa-check"></i><b>5</b> Network Effects</a></li>
<li class="chapter" data-level="6" data-path="diffusion-of-innovation-and-writing.html"><a href="diffusion-of-innovation-and-writing.html"><i class="fa fa-check"></i><b>6</b> Diffusion of Innovation and Writing</a><ul>
<li class="chapter" data-level="6.1" data-path="diffusion-of-innovation-and-writing.html"><a href="diffusion-of-innovation-and-writing.html#diffusion-of-innovation-theory"><i class="fa fa-check"></i><b>6.1</b> Diffusion of Innovation Theory</a></li>
<li class="chapter" data-level="6.2" data-path="diffusion-of-innovation-and-writing.html"><a href="diffusion-of-innovation-and-writing.html#social-network-lifecycles"><i class="fa fa-check"></i><b>6.2</b> Social Network Lifecycles</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>7</b> Conclusion</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Writing Macroscopes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="writing-macroscopes" class="section level1">
<h1><span class="header-section-number">3</span> Writing Macroscopes</h1>
<p>Questions regarding software development and the role of computer programming for rhetoric and writing studies have regularly re-emerged in various forms over the last four decades. In the 1980s scholars interested in cognitive and procedural aspects of the writing process programmed key logging software to use in research. These tools were built to help study the various ways writers revise while writing, and think-aloud protocols were supplemented with keystroke data collected while writers worked on their computers (Flinn 45). As Internet writing technologies expanded through the 1990s, software development with hypertext and production based tools started to emerge for rhetoric and writing studies. Paul J. LeBlanc’s Writing Teachers Writing Software explains how, during this time, software development was motivated by social and political concerns, along with technological and research possibilities. LeBlanc quotes Andrea Lunsford’s 1991 MLA keynote address: “Who will create and control the programs, the networks (on which many are, ironically, silenced and excluded), the architecture of interactive fiction, the prototypes of virtual reality?” As LeBlanc explains, “If we wish to take a proactive role in the shaping of electronic literacy, software design should be as mainstream an activity for composition professionals as teaching a writing class, conducting a research study, or writing an article” (29). In recent years, conversations regarding programming have argued for a rhetorical understanding of code in order to integrate more technical approaches to writing instruction (Cummings), or they have turned toward a discussion of design and production in a post-web 2.0 environment (Arola).</p>
<p>Post-Web 2.0 writing environments such as cloud word processors, online design and editing systems, mobile and touch-screen applications, and countless social/sharing networks immensely expand the possibilities for circulation studies. Such ongoing expansion in the tools and technologies available for creating and circulating digital artifacts have led to enormous amounts of data—what is often referred to as “Big Data.” The problem with the frequent use of the phrase “Big Data” is that it also implies increasingly democratized access. Vast amounts of data are constantly being digitized and created, but much of the data of interest to circulation studies remains difficult to access, owned by corporations, and resold at prices too high for many scholars to afford. Social network data, in particular, continues to increase in both value and analytical power as more and more users around the world interact and share content. Therefore, even while such data is seemingly everywhere, access restrictions to social network data have severely bottlenecked the methods that may be employed to research the texts, images, and other digital artifacts that are in circulation.</p>
<p>I have contributed to a software project that directly confronts the problem of data access for social media. As the co-creator of MassMine, along with Nicholas Van Horn, I have been fortunate to experience, from the beginning, the development of tool that is intended to support humanities research. MassMine is an open source research tool created to help scholars collect and analyze social network data. MassMine was recently awarded a level II startup grant from the National Endowment of the Humanities, and a stable version of the software has been actively tested on the supercomputer cluster at the University of Florida for over a year. The software currently pulls data from Twitter and Wikipedia, and the funding made available by the NEH grant is supporting new development to add Facebook and other networks as data sources. Drawing on my experience assisting in the development of MassMine, this chapter will consider some of the issues involved in accessing social network data. Methodological challenges emerge when developing software for academic research, and they raise questions about the relationship between technologies and the types of research they afford.</p>
<p>The first section of this chapter provides an introduction to data available through Twitter and explains its value for research. Facebook and many other social networks collect/produce data similar to Twitter, and thus Twitter data provides a useful introduction to social network data in general. The second section of this chapter explains the current nature of the access restrictions for social networks, and how MassMine uses a sustainable system of data collection to confront this issue. However, access is only one aspect of the problem posed by studying social network data. Just as access restrictions create research bottlenecks, the potential expansion of access and the new approaches that may be developed as a result present challenges as well. When developing software to conduct new forms of research for an emerging field, the assumptions and limits of that field quickly come into question. While MassMine was initially developed to support circulation studies research and to facilitate access to data, the final section draws on data science to explain how software development for data-intensive research may transform the very methods that motivated such projects in the first place. My experiences with MassMine have convinced me that circulation studies has three pressing issues that must be confronted in order to utilize its fullest potential: First we need to continue to improve our understanding of the circulation data that is available. Next, we need better access to that data. Access is not limited to the physical, material access to data, but it also includes the availability of methods for processing and analyzing data. Finally, integrating data analysis with circulation research will require that we expand and transform many of our current research methodologies.</p>
<div id="social-network-data" class="section level2">
<h2><span class="header-section-number">3.1</span> Social Network Data</h2>
<p>Twitter aggregates and stores every tweet ever written, and it has been one of the most influential networks in making social network data widely applicable to many various forms of research. One of the more frequent critiques of Twitter for social media research is the constraint of the 140 character limitation for tweet texts. It seems like very little data about trending1 topics and their circulation among users and locations would be available through such a limited framework. This section, however, investigates how such micro texts can produce massive amounts of data, and, more importantly, how the data functions. Consider the following example:</p>
<p>In the small example above there is the primary text included in the tweet: “Birds of a feather tweet together. <span class="citation">@twituser887</span> #tweettweet exampleURL.com.” This micro-text only used 75 characters of the available 140 to communicate the short message, but the semantic message of the text is only one small component of the overall available data—the data available in this one single tweet is far greater than the message it attempts to communicate.</p>
<p>One of the key components of non-message data included in the tweet itself is the categorical tag: #tweettweet. The word “trend” is often associated with #hashtags—the user tagging system that categorizes tweets into broader movements, discussions, or concepts. In the case of a trend like #blacklivesmatter, for example, the #hashtag functions as intended—it allows people who tweet about a topic to categorize their messages with other similar messages around a similar topic. However, #hashtags are also used to serve a semantic function in the message text itself–what Alice Daer, Rebecca Hoffman, and Seth Goodman refer to as a “metacommunicative” function (Daer). For example, a student of mine spent last semester researching the trend: #salty. On the surface, this appears to be another item of Internet slang efficiently moving its way into English vernacular, but what is interesting about this trend is its function both semantically and categorically (and many #hashtags operate this way). Certainly, all words—in both print and digital media—function both semantically and categorically, but #hashtags allow users to place additional categorical emphasis on specific terms. In the example “Birds of a feather” tweet above, for instance, “#tweettweet,” acts more as a traditional categorical tag that is placed after the message text, but #salty often appears within the message itself and takes on a rhetorical function: “<span class="citation">@username</span> was #salty about his recent disagreement with <span class="citation">@person</span>.”</p>
<p>One of the benefits of Twitter’s network, for researching trends, is that it provides users with data on #hashtag trends as well as providing data on the organic non-hashtag trends surrounding the most frequent words or phrases currently in circulation. For example, a trend may start from common terms relating to current events, the names of public figures attached to a trending news story, or any other slogan or phrase that appears in a large number of tweets at any particular location within Twitter’s system. Twitter provides trend data on 63 locations across the United States and many others worldwide. While many researchers already know which trend or topic they want to research, location data helps identify how much that particular trend circulates within/among specific locales. Indeed, many trends are important because of the trend’s content or the events that may have motivated the trend in the first place. However, location-based trend research can show how much attention a particular figure, event, or movement is gaining within a social network. Such research could supplement ethical and/or political claims of a current event’s value by showing that a particular figure, event, or slogan has gained a significant amount of attention2. There are three other important data types that are useful for circulation studies research: URLs/images, <span class="citation">@usernames</span>, and timestamps. While #hashtags allow for categorization and linking within a network, URLs allow users to link their micro-texts to external contexts and sites (platforms like Facebook work to make external linking difficult in order to keep user sharing and activity within their singular network). Often users will respond to a news article or blog post and share its URL. This form of URL sharing has been available through email since the invention of the Internet. However, many researchers are not aware of how difficult it is to archive and track this type of data.</p>
<p>This problem also exists for image/photo research since images are often hosted on servers outside of social networks, and only the URL link to the hosting source is shared. For images that are uploaded to Twitter’s own network, these images are distributed through Twitter’s “t.co” system for shortening URLs. When Tweets with attached images are viewed on Twitter’s network or through their mobile application, then the actual images appear at the bottom of the Tweet. But when raw tweet data is collected from Twitter, images appear as “t.co” URLs. URLs present a unique problem because they are always shifting and changing. For example, a user may share an image hosted at another site or network—like Tumblr or Instagram—but the Tumblr image may be deleted or changed, and then that URL included in a Twitter message will no longer link back to the image source. The same can happen with news stories and blog posts. The blog site may change revise an article and change its web address, and the URL included in the archived tweet will then be broken. Activities as common as software updates disrupt relational addresses so that URLs no longer link appropriately to intended content. There are no easy solutions to these problems, but URLs remain an important aspect of social network data (and Internet data in general) because of how they link to objects outside of the network.</p>
<p>Another valuable source of information available through Twitter is the demographic data provided by <span class="citation">@usernames</span> based on data collected from user profiles. Even though Twitter only provides <span class="citation">@username</span> data on public accounts, circulation studies researchers must still consider privacy issues and the research ethics involved with their project when considering uses for this type of data. This means that researchers should get IRB (institutional review board) approval for their research when collecting <span class="citation">@username</span> data, but also, on a broader scale, circulation studies must take part in the ongoing theoretical investigations of the shifting concept of “public” space occurring in social networks. Or, put differently, scholars should work to understand the erasure of “privacy.” One way to investigate this theoretical issue is to collect user data in order to better understand what user information is already available for aggregation and analysis, because a better understanding of <span class="citation">@username</span> data could support well-informed critiques that raise the awareness of how user surveillance functions within social networks. However, privacy presents a multilayered problem because privacy protections may also conceal violent users who seek to “troll” or bully other people. For example, in the case of the #gamergate trend, malicious users were setting up fake accounts as females and people of color in order to sabotage critical awareness-raising aspects of the trend. Privacy and surveillance issues for social network data will continue to provide ethical and political challenges, and circulation studies must remain aware of these issues as they continue to evolve and actively contribute to the ongoing conversation.</p>
<p>Finally, the most important yet seemingly innocuous data available in tweets is the time/date of delivery. Often, the time and date matter little for the audience of an individual tweet—most of the readers of this chapter likely did not pay attention to the time and date for the “Birds of a feather” example tweet. But this data type—the “timestamp” as it is commonly called—controls the way tweets are ordered and aggregated, and it is the most significant aspect of how groupings of similar tweets become trends. It is also one type of data that users do not entirely control—many users give almost no consideration to timing whatsoever. Yet, even if many users save posts written from the night before, in order to post them during more attentive times the following day, users cannot control how their tweets are ordered with other tweets in their followers’ feeds. Users can turn off their geolocation data for tweets, they can create fake profile information, and they can write nothing but gibberish tweets that are meaningless—but the timing of their tweets and their activity over a period of time cannot be changed or adjusted. Timestamps are added to tweets after the fact, and users can only control when they hit the “post” button. When MassMine collects and exports Twitter data, the timestamp is its own column of data. Timestamp data, when combined with other forms of data and analysis, offer some of the most powerful possibilities for circulation studies. Location data is often only relevant when placed within the temporal contexts provided by timestamp data. In fact, the combination of location data and temporal data, when investigated through the scope of given trend or topic, makes social network data a crucial resource for understanding where, when, and how digital artifacts are circulating.</p>
</div>
<div id="data-access-and-collection" class="section level2">
<h2><span class="header-section-number">3.2</span> Data Access and Collection</h2>
<p>Given the immense value of social network data, one of the first issues addressed by MassMine was the development of sustainable forms of access and data collection for research. There are three core issues that often restrict access for researchers interested in collecting and studying the vast data made available through social networks: (1) the high cost of access to data-resellers who package and charge high fees for licensed data from social networks and web applications; (2) if data access is paid for through popular providers, pre-filtered and already-visualized data are often only available through inflexible web-browser tools as opposed to providing full access to “raw” data; (3) the preset filters and already-made data visualizations focus primarily on marketing and brand management issues and this restricts research questions to those that serve a limited commercial scope. Radian6, GNIP, and Topsy are three of the top data re-sellers—owned by SalesForce, Twitter, and Apple respectively. These companies charge fees for access to data because businesses are willing to pay for the analytical power the data provide. There are “free” providers, such as SumAll, that provide limited overviews of data in order to try and convince users to pay for the “premium” access to more comprehensive analytics. Many other free providers will track or map #hashtag data for users, but most of these tools lack sufficient export functionality, offer no historical data, and are limited to pre-visualized cloud environments. Whether free or not, most of the tools available focus on business analytics and marketing research because the profit incentives in analytics software development remain high. Therefore, the types of analysis available through the browser tools that data re-sellers create for their users are not intended to answer the broad, far ranging, and open-ended research questions investigated in circulation research.</p>
<p>Massmine embraces the use of APIs (application programing interfaces) to collect data from social networks. At no cost to programmers, APIs provide access to broad social network data for application and software development. API data may be “freely” collected, archived, and analyzed for academic research. Companies like Twitter, Facebook, and Google, among many others, create APIs for their networks because they want users and outside developers to create software that interacts with their site. For example, Instagram started as a photo sharing application that edited photos and forwarded them automatically to other social networks through the access provided by APIs. Instagram’s creation was made possible because of API access to user accounts and network data. Facebook benefited from the development of Instagram by having another software systematically increase the photo content on their network, and therefore large companies like Facebook have a financial motivation to continue to provide free access to network data through their public APIs. The data available through APIs may be accessed without paying fees, but this data is usually still licensed and owned by the social networks providing the access. While collection for research is allowed, users cannot broadly and publicly share this data without violating user agreements. Fortunately, many companies allow users who access APIs to use the data free of charge for software programming and academic research because the companies directly benefit from this continued external development and publicity. Massmine thus heavily relies on application programming interfaces to collect data for research. APIs, of course, are not without limitation themselves, but free public access makes APIs a good starting point for developing research software for circulation studies.</p>
<p>MassMine’s initial accomplishment as a project was systematically collecting Twitter Rest API3 trend data over a period of time to build a historical archive for circulation research. Since it is not possible to simply access historical trend data directly from Twitter and then analyze it, without paying high fees, MassMine’s initial code was written to collect and archive location-based trend data over a 3 month period. This approach to data collection allows researchers to create their own historical archives that can be used for research once enough data is collected. The initial code for MassMine collected the top 10 trends every 5 minutes for each of the locations we researched—this means that 100 lines of data were added to our archive every 5 minutes. After doing this initial test collection, there was enough historical data in the archive created by MassMine to begin analyzing trends. The amount of data collected was not “big” data by any available data science metric, but it was certainly plenty of data for an exploratory project in circulation studies. A single line of trend data includes the name of the trend, its location, and its time stamp (when it was in the top 10 for its particular location). Twitter uses the WOEID (Where On Earth ID) location system for its trends and situates the trends in a location based on the profile locations provided by its public users. MassMine collects this data, processes it, and exports it in a CSV file (the most universal/open file format for spreadsheets and statistical software) for users.</p>
<p>For accessing data about a specific trend through Twitter’s APIs, there is the Rest API, already mentioned, and then there is Twitter’s Streaming API which provides a live, streaming random sample of all tweets pertaining to a specific search criteria, for as long as a user requires for a collection. The small amount of historical Tweet data provided by the Rest API is useful if a researcher has realized after-the-fact that an important trend has already begun, but for completing research on specific trends the Streaming API provides drastically more data. For example, MassMine was used to mine data from Twitter for this year’s Academy Awards using the popular “#oscars” trend for data collection. There is usually a lot of Twitter activity leading up to the Academy Awards because of people’s responses to nominees or users tweeting about their predictions for awards. So before starting a Streaming API collection on the morning of the Academy Awards with MassMine, a quick Rest API collection was conducted in order to pull whatever historical information on the #oscars was available. The Rest API only returned one day’s worth of data. This is another limitation of the Rest API—the larger a trend, the less of a historical window allowed by the Rest API. However, by systematically collecting data over time through the Streaming API, 1.4 million tweets pertaining to the #oscars were collected in only 6 hours with MassMine.</p>
<p>The main limitation to this approach is that researchers must pay attention to social media and be prepared to start a data collection before or soon after a trend begins. Access limitations are not a common problem for humanities scholars—research is often conducted on artifacts that are already available in a library or another publicly curated archive that provides full historical access. When using APIs to collect social network data, however, scholars must collect streaming data as it becomes available. In this way, social network research is closer to scientific research than humanities research. Like researchers who study the migration patterns of birds or astronomical phenomena occurring in a nearby solar system—if a researcher is not prepared to observe a social trend shortly after it begins, they must wait until that trend or another similar trend occurs at a later point in time. This limitation is caused by the access restraints to historical data, but if researchers are prepared to collect data as trends occur and if they pay attention to relevant issues on social media, then this limitation does not need to impede effective research.</p>
<p>Fortunately, trends like the #oscars have predictable patterns, or the date of their occurrence is known far in advance. But for more organic trends, like #blacklivesmatter, it is difficult to know when to begin a data collection. MassMine had numerous scholars contact us after the protests in Ferguson, Missouri to see if we could help them acquire historical data after the protests related to Michael Brown had subsided, but we had to inform them of the limitations on historical data. Certainly, it can be discouraging when historical data is only available through paid data services, but #blacklivesmatter has had numerous important protests since then and the trend has remained very active. If the scholars started collecting data at the time they initially contacted us, then they would have a significant dataset that includes the more recent protests in Baltimore, Maryland and many other recent developments in the movement. Yet, even for more predictable trends like the #oscars, less predictable events occur that allow for alternative/associated trends of greater social importance—the award-winning and influential Mexican directors from the 2014 awards, and the focus placed on feminism, white-privilege, and wage equality were all associated trends that took on a momentum of their own. All of these events could have been studied in isolation, shortly after occurring, or they could have motivated data collections pertaining to other similar trends and movements already underway.</p>
<p>Regardless of the type of social network data collected by researchers, Twitter’s recent announcement to remove “firehose” access for third-party data resellers means that API data will likely become the standard for free and public data pertaining to social networks, and many of the third-party data resellers already sell the freely-available API data as if it were the standard. The companies and individuals that pay to use the tools provided by data resellers are willing to pay for data that is otherwise free because they want the analytics that result from the data access (storage, processing, analysis, and visualization of the data). In order for access to expand and continue to improve for academic research, scholars must locate and support open access initiatives pertaining to social networks. This includes paying close attention to changes in “terms of service” agreements for networks, and working to ensure that users have adequate access to the data they help create. While MassMine has embraced APIs as a sustainable model for accessing social network data, other ways of collecting and archiving social network data must be developed and established. If the broad academic community of scholars in higher education begin fostering widespread support of open access to social network data, then effective research alternatives to the profit-model of social network analysis will continue to emerge.</p>
</div>
<div id="circulation-analytics" class="section level2">
<h2><span class="header-section-number">3.3</span> Circulation Analytics</h2>
<p>Although the work to improve access is far from over, as access improves the questions regarding social network data transition from external questions about access to internal questions about our own research methodologies. What do we do with the data? Will integrations of data-intensive research and writing studies methods lead to reciprocally transformed research? Are we open to such transformations? A recent example of methodological development in circulation studies is Laurie Gries’ iconographic tracking method, which was invented by Gries to study the circulation and remix of the Obama Hope image created by Shepard Fairey in 2008 (333). Gries’ work focuses on an iterative reciprocity between collection and analysis, and she uses strategies for image-tracking and data-collection workflows to track the iconic imagery. As Gries explains, “the methodological framework and research methods necessary to study rhetoric in motion still need to be developed and distributed.” (346). While Gries’ project is not specifically a software project, the research approach she develops for collecting, organizing, and tracking iconic images challenges us to ask what is the difference between methodology and research software? In the case of Gries’ groundbreaking study, the question is answered by asking who—or more specifically, what—is doing the work of collecting, organizing, and tracking the iconographic images. Many of the systematic aspects of circulation research that are currently undertaken by human activity—the copying of documents or images, saving them to folders, and then tagging the files with metadata—may be replaced by a computer program that facilitates this activity and makes it systematic and reproducible. An immediate barrier to such transitions is a need for more training and education in effective programming practices, but this poses less of a problem than the potential methodological barriers to embracing procedural/operational data-intensive analysis in circulation studies.</p>
<p>A New York Times article from last year, titled “For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights,” explains that, even though many tools claim to provide ready-made data analytics, “far too much handcrafted work—what data scientists call ‘data wrangling,’ ‘data munging’ and ‘data janitor work’—is still required.” According to the article, data scientists “spend from 50 to 80 percent of their time mired in this more mundane labor of collecting and preparing unruly digital data, before it can be explored for useful nuggets.” Laurie Gries describes a data wrangling process in her research as well—where iconographic tracking begins with “data hoarding,” then moves to a second step where the researcher begins “assembling data into a collection” (339), and then a third step that acts as a “recursive process between tracing, data mining, and assembling a collection.” As Gries explains, this “recursive process generates and organizes data so that scholars can decide which collectives they want to investigate on a micro-level scale during the next research phase” (340). Similar to Gries’ work, the exploratory aspects of data janitorial work for MassMine—the cleaning of raw data for eventual insights and analyses—has led to some of the newest developments for the project. Given how these emerging methods in circulation studies position the field much closer to data science, the potential methodological barriers to data-intensive research may be much less than many scholars would assume.</p>
<p>In 2009 Microsoft commissioned an open-access edited collection called The Fourth Paradigm in honor of the late Jim Gray. Computer scientist, database researcher, and winner of the Turing Award for database and systems research, Gray devoted the end of his career to the Sloan Digital Sky Survey, “the largest database available that consisted entirely of public data,” in order to advance public access initiatives and data science methodologies in the traditional sciences (Stonebraker). In The Fourth Paradigm they transcribe one of the last public lectures given by Gray titled, “Jim Gray on eScience: Transforming the Scientific Method.” Gray explains:</p>
<blockquote>
<p>The world of science has changed, and there is no question about this. The new model is for the data to be captured by instruments or generated by simulations before being processed by software and for the resulting information or knowledge to be stored in computers. Scientists only get to look at their data fairly late in this pipeline. The techniques and technologies for such data-intensive science are so different that it is worth distinguishing data-intensive science from computational science as a new, fourth paradigm for scientific exploration. (xix)</p>
</blockquote>
<p>What Gray explains regarding data science is very similar to what Gries explains regarding infographic tracking and what we have discovered with MassMine working with social network data: Circulation studies scholars conduct research on trends, documents, and digital artifacts after they have been invented, and these objects of study only become available for collection and analysis after they have been delivered to a particular network for distribution and circulation. Furthermore, the trends, documents, and digital artifacts–when they are aggregated, collected, and archived systematically–are just as measurable as any other phenomenon. As the first section of this chapter explains, the combination of location and temporal data, when investigated through the scope of a given trend or topic, provides the necessary components to apply measurable procedures to digital artifacts.</p>
<p>As more comprehensive data analytics are developed (for processing, analyzing, and visualizing circulation data about networked trends, documents, and digital artifacts), projects like MassMine challenge scholars to consider the extent to which they are prepared to start using measurable procedures to supplement rhetorical analysis. MassMine was motivated by the access limitations caused by issues with social network data, but as the access restrictions have become less of an issue, the problems are now the limitations provided by the methodologies that motivated the work in the first. In other words, as access to social network data improves, what should circulation studies do with the data? How should the data be processed, analyzed, and visualized in a way that circulation studies can use? In many ways, circulation studies is coming full circle to the methods attempted in think-aloud protocols and cognitive research mentioned in the introduction to this chapter. The differences are that the subject is no longer strictly pedagogical, the scale is exponentially larger than the average number of students in a college classroom, the writing is occurring within/across multiple, overlapping digital networks, and the process is operational rather than cognitive. As Sidney Dobrin explains in Postcomposition, “This new mantra urges researchers to step beyond the limits of thinking about writing in terms of classroom application and observation, calling instead for research that begins to tear down the very boundaries of the field in order to develop more useful, accurate theories of writing” (190).</p>
<p>This problem of “what do we do with the data?” is not unique to circulation studies, of course. Dynamic data analytics tools are embedded everywhere. CNN broadcasts often include news anchors using large touch-screens to interact with dynamic infographics. Data visualizations are becoming far more commonplace in blogs and internet journalism. Indeed, even sports and entertainment media are obsessed with advanced analytics and data metrics. The problem is not their ubiquity, but that they are often impenetrable–providing no access to underlying data or the processes that led to the eventual production of the data visualizations4. But even when access to data and processing is permitted, what percentage of the audience has the adequate statistical and data literacies to be critical of the final visualized analysis? To bring this question more specifically back to circulation studies, what percentage of scholars in rhetoric and writing studies have the adequate training to judge whether methods of data processing and data visualization fit (or adequately supplement) critical and rhetorical standards of peer review? While these problems are not unique to circulation studies, how we decide to complement already available methods with data-intensive research is a question that needs explicit and ongoing interrogation.</p>
<p>Fortunately, circulation studies is not the only field asking these questions. Data science is becoming more of a ubiquitous methodology than a strict discipline of its own. Certainly, there are data science and computer science departments in many universities, and there are problems and discourses unique to those designations. However, as Jim Gray explains in The Fourth Paradigm, “We are seeing the evolution of two branches of every discipline,” and he provides examples from ecology and biology to explain the influence of data-intensive methodologies across academe:</p>
<blockquote>
<p>If you look at ecology, there is now both computational ecology, which is to do with simulating ecologies, and eco-informatics, which is to do with collecting and analyzing ecological information. Similarly, there is bioinformatics, which collects and analyzes information from many different experiments, and there is computational biology which simulates how biological systems work… (xix)</p>
</blockquote>
<p>Gray’s observation is meant to explain how the fourth paradigm is finding new forms of methodological variation in many diverse fields of study. There is no need to create strict divisions in circulation studies between scholars interested in data-intensive research and scholars who deploy rhetorical methods of analysis–these methods should be understood as complementary not contradictory.</p>
<p>In Rhetoric and the Digital Humanities, Jim Ridolfo and William Hart-Davidson describe the overlapping space among rhetoric, writing studies, and the interdisciplinary field of the digital humanities as “a broader and deeper set of shared interests and intellectual commitments.” Literary scholars in the digital humanities have been developing methods for conducting macroscopic analysis of large corpora of literary data. Matthew L. Jockers, a digital humanities scholar known for his Macroanalysis: Digital Methods and Literary History, wrote a companion textbook explaining how to complete macro-analyses on large corpora of literary texts. Jockers’ Text Analysis in R for Students of Literature is full of tutorials on how to use the R language to mine texts for literary analysis. There is a significant amount of knowledge that circulation studies can draw from the digital humanities in terms of DH’s ongoing work with textual macroanalysis. In fact, many of the text-mining methodologies already well-established in DH, such as topic modeling and natural language processing, are immediately useful for circulation studies research. MALLET, for example, is an open source research software for text mining, natural language processing, and topic modeling that circulation scholars could use to analyze text collected from social networks. The DiRT directory (www.dirtdirectory.org) is a large archive of many similar tools and resources that have either been developed within the digital humanities or that have been retrofitted for humanities research. While some of the tools are more specifically focused on literary or historical research, others are more generally applicable to broader forms of analysis that may be of use to circulation studies.</p>
<p>MassMine also uses the R programming language for analysis and draws on methods comparable to Jockers’ for text mining and natural language processing. What makes R such a valuable resource for scholars interested in applying data-science methodologies to many broad areas of research are the thousands of libraries available for various types of quantitative and statistical data analysis. Often, people who are new to learning how to program assume that code is written from scratch, and that a high level of mathematical knowledge is required. Programming libraries, however, allow users to take portions of already-written code that accomplish specific tasks, and plug them into their own programs as building blocks. For example, the TM package (R often calls its libraries “packages”) in R has many pre-built tools for assisting scholars in completing quantitative and statistical analyses of textual data. TM stands for “Text Mining.” The TM package will read into R a large corpus of documents, and turn them into a document-term matrix. This task can be completed with no more than four lines of code because the TM package completes this conversion from raw text to the mathematical construct of the matrix. Once the matrix is created, word frequencies and correlations can be calculated across all the documents in the corpus that was used to create the matrix. MassMine users can deploy this same technology to calculate word frequencies and correlations in large collections of text taken from tweets.</p>
<p>In order to seriously confront the issue of how to complement/supplement already available methods with data-intensive research, more scholars in writing studies and circulation studies will have to consider learning to program as a core skillset. However, coding should not become a gate-keeper for the field. A long standing debate in the digital humanities is whether or not learning to program should be required in order to self-identify as a full-fledged “DH” scholar. Matthew K. Gold addresses this issue in his introduction to Debates in the Digital Humanities, but these gate-keeper disciplinary concerns fall short of addressing Lunsford’s broader political and social concerns for virtual spaces and writing networks. Coding, programming, and software development should not be a way to limit any field of study—they should inclusively function to expand the available objects (and/or methods) of research. Too often, writing scholars are forced to retrofit outside tools for their research, rather than having software designed for their particular research needs. To some extent, retrofitting outside tools will always take place in rhetoric and writing studies—especially when research is about newly emerging writing and sharing technologies. But when it comes to data-intensive social media research (and other similar forms of research), tools that heavily pre-process or restrict access make certain types of research very difficult, if not impossible. Identifying these problem areas, and building new tools and software to confront these problems, could provide an inventive nexus of development for circulation research.</p>
<p>Therefore, the question raised at the beginning of this section–“what is the difference between methodology and research software?”–is rhetorical. The primary difference between the two is the software, the programs. Intersections of software development and data science methodologies encourage collaborative and open communities where scholars work together to determine which aspects of any given research methodology may be systematized and made reproducible through the writing of programming scripts and the development of software. But the collaborative community of which circulation studies is a part is much broader than disciplines of writing studies and rhetoric. Data scientists from many various disciplines are regularly developing programming libraries and packages that complete many of the quantitative and statistical analyses that are applicable to circulating trends, documents, and digital artifacts. However, these tools are only building blocks–they are not ready-made technologies that easily accept any data and simply produce a final analysis. Rather, much work remains in circulation studies to bridge the gap between already available statistical tools and the circulation data that scholars need to access and study. Scholars must be willing to respond to LeBlanc’s call that software development become a mainstream activity for networked writing research. However, as this section argues, coming to terms with data-intensive methodologies is just as important as making software development a mainstream activity. Circulation studies has the opportunity to develop new and exciting integrations of rhetorical and data-intensive methodologies, and it starts with a willingness to transform the very methods that have made the study of circulation such an exigent field of research in the first place.</p>
<p>As for MassMine, the project has taken on a life of its own. The software’s core API data collection technology, which was originally written in R, has been completely rebuilt in C to allow for greater portability among operating systems and computing environments. Much like writing an article for publication, organizational structures and decisions that made sense in the early stages of drafting must be changed or removed as writers revise for final submission. As the MassMine team works toward the version 1.0 release in fulfillment of the NEH grant, and as the user community grows and suggests changes and upgrades, the development process will undoubtedly lead to future changes in the software. R will continue to complete the processing and analysis for MassMine, but the rebuild of the core is intended to let users disregard R and use whatever analytics they prefer (or, hopefully, build themselves). There are seemingly countless ways to combine, compare, process, and analyze social network data, and programmers could hypothetically write customized analytics for each new form of analysis a researcher develops. Every time a new analytics tools is created and shared among a community of researchers, the rest of the community benefits. As a result, research is expanded and transformed from the increased capability of the entire community. MassMine, as an open source project, seeks to not only exemplify this model, but also to provide programmers with the tools to pursue their own data-intensive projects. Questions regarding the circulation of social network trends motivated the initial development of MassMine, and many new questions regarding circulation continue to emerge as resolution improves. Like a pixelated image slowly coming becoming clearer, we intuitively focus our own vision to improve what we see–without knowing whether the image itself must be enhanced, or our own ability to focus our sight. In the case of social network data, we must do both. Circulation studies builds upon familiar methodologies derived from rhetoric and writing studies, while encouraging inventive expansions into necessarily unfamiliar research.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="writing-to-scale.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="network-descriptions.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
